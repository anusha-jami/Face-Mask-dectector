{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8639d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import UnidentifiedImageError\n",
    "import PIL\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136c9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Sequential([\n",
    "    Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(100, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f429a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1315 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"C:/Users/anush/Downloads/face-mask-dataset/Dataset/train\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7572c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 194 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_DIR = \"C:/Users/anush/Downloads/face-mask-dataset/Dataset/test\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=10, \n",
    "                                                         target_size=(150, 150))\n",
    "checkpoint = ModelCheckpoint('model2-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06961fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\an\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1935: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "132/132 [==============================] - 99s 729ms/step - loss: 0.4086 - acc: 0.8008 - val_loss: 0.1449 - val_acc: 0.9485\n",
      "INFO:tensorflow:Assets written to: model2-001.model\\assets\n",
      "Epoch 2/10\n",
      "132/132 [==============================] - 95s 716ms/step - loss: 0.2213 - acc: 0.9202 - val_loss: 0.1511 - val_acc: 0.9536\n",
      "Epoch 3/10\n",
      "132/132 [==============================] - 99s 750ms/step - loss: 0.2041 - acc: 0.9270 - val_loss: 0.1359 - val_acc: 0.9588\n",
      "INFO:tensorflow:Assets written to: model2-003.model\\assets\n",
      "Epoch 4/10\n",
      "132/132 [==============================] - 95s 720ms/step - loss: 0.2053 - acc: 0.9285 - val_loss: 0.1193 - val_acc: 0.9485\n",
      "INFO:tensorflow:Assets written to: model2-004.model\\assets\n",
      "Epoch 5/10\n",
      "132/132 [==============================] - 95s 719ms/step - loss: 0.1722 - acc: 0.9422 - val_loss: 0.1193 - val_acc: 0.9536\n",
      "INFO:tensorflow:Assets written to: model2-005.model\\assets\n",
      "Epoch 6/10\n",
      "132/132 [==============================] - 95s 715ms/step - loss: 0.2061 - acc: 0.9163 - val_loss: 0.1273 - val_acc: 0.9639\n",
      "Epoch 7/10\n",
      "132/132 [==============================] - 98s 741ms/step - loss: 0.1805 - acc: 0.9331 - val_loss: 0.1067 - val_acc: 0.9639\n",
      "INFO:tensorflow:Assets written to: model2-007.model\\assets\n",
      "Epoch 8/10\n",
      "132/132 [==============================] - 95s 716ms/step - loss: 0.1560 - acc: 0.9483 - val_loss: 0.0973 - val_acc: 0.9639\n",
      "INFO:tensorflow:Assets written to: model2-008.model\\assets\n",
      "Epoch 9/10\n",
      "132/132 [==============================] - 94s 711ms/step - loss: 0.1403 - acc: 0.9483 - val_loss: 0.0928 - val_acc: 0.9639\n",
      "INFO:tensorflow:Assets written to: model2-009.model\\assets\n",
      "Epoch 10/10\n",
      "132/132 [==============================] - 95s 720ms/step - loss: 0.1551 - acc: 0.9475 - val_loss: 0.1847 - val_acc: 0.9021\n"
     ]
    }
   ],
   "source": [
    "newmodel_1 = model.fit_generator(train_generator,\n",
    "                              epochs=10,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21e74bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: newmodel_1.h\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"newmodel_1.h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b982b983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('C:/Users/anush/newmodel_1.h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3fc200f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 100)     2800      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 100)       90100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 100)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 129600)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 129600)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                6480050   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 6,573,052\n",
      "Trainable params: 6,573,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict={0:'without mask',1:'mask'}\n",
    "color_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('C:/Users/anush/Downloads/face-mask-detector-project/haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(150,150))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        #print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if Esc key is press then break out of the loop \n",
    "    if key == 27: #The Esc key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94c11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c29a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
